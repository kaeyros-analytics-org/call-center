url = "lda/index.html"
lda <- tags$iframe(src=url, height=400, width=400)
lda
runApp()
install.packages("devtools")
library("devtools")
install_github("sentiment", "andrie")
install_github("sentiment", "andrie")
library("sentiment")
install.packages("sentiment")
install.packages("sentimentr")
sentiment(c("There is a terrible mistake in this work", "This is wonderful!"))
mytext2 <- get_sentences(c("There is a terrible mistake in this work", "This is wonderful!"))
library("sentimentr")
mytext2 <- get_sentences(c("There is a terrible mistake in this work", "This is wonderful!"))
emotion(mytext2)
emo_words <- extract_emotion_terms(mytext2)
emo_words
################ ANALYSE DES SENTIMENTS ###################################"
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
library(tidytext)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
tidy_books
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
install.packages("textdata")
library(textdata)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(sentiment = positive - negative)
jane_austen_sentiment
View(jane_austen_sentiment)
################ ANALYSE DES SENTIMENTS ###################################"
# utilisation du dictionnaire des sentiments financiers créé par Loughran et Mcdonald (2011)
devtools::install_github("quanteda/quanteda.sentiment")
require(quanteda.sentiment)
data_dictionary_LoughranMcDonald[c(1, 3)]
require(quanteda)
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks
toks <- tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2)
dfmt <- dfm(toks)
dfmt
View(dfmt)
dfmt_dict <- dfm_lookup(dfmt, data_dictionary_LoughranMcDonald[c(1, 3)]) / ntoken(dfmt)
dfmt_dict
dat_dict <- cbind(convert(dfmt_dict, to = "data.frame"), docvars(dfmt_dict))
dat_dict
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks
toks <- tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2)
dfmt <- dfm(toks)
dfmt
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks <- tokens_remove(toks[[1]], pattern = stopwords("en"), min_nchar = 2)
toks[[1]]
toks <- tokens_remove(toks[[1]], pattern = stopwords("en"), min_nchar = 2)
dfmt <- dfm(toks)
dfmt
dfmt <- dfm(toks[[1]])
corp
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE,
remove = stopwords("en"), min_nchar = 2)
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks <- tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2)
dfmt <- dfm(toks)
dfmt
ntoken(dfmt)
dfmt_dict <- dfm_lookup(dfmt, data_dictionary_LoughranMcDonald[c(1, 3)]) / ntoken(dfmt)
dfmt_dict
toks <- tokens(corpus(df_total_en_clean_client$value), remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks
toks <- tokens(df_total_en_clean_client$value, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE)
toks
toks <- unlist(tokens(df_total_en_clean_client$value, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE))
toks
corp
corp
toks <- unlist(tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2))
toks <- unlist(tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE))
toks
toks <- tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2)
toks <- unlist(tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, remove_symbol = TRUE))
toks <- tokens_remove(toks, pattern = stopwords("en"), min_nchar = 2)
names(toks) <- NULL
toks
names(toks) <- NULL
dfmt <- dfm(toks)
################ ANALYSE DES SENTIMENTS ###################################"
# utilisation du dictionnaire des sentiments financiers créé par Loughran et Mcdonald (2011)
require(seededlda)
install.packages("seededlda")
################ ANALYSE DES SENTIMENTS ###################################"
# utilisation du dictionnaire des sentiments financiers créé par Loughran et Mcdonald (2011)
dfmt_grp <- dtm %>%
dfm_trim(min_termfreq = 0.5, termfreq_type = "quantile",
max_docfreq = 0.1, docfreq_type = "prop") %>%
dfm_group(company)
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
shiny::runApp()
root <- getwd()
path_data <- file.path(root,"data")
path_data_en_AFB23 <- file.path(path_data,"en_info_data_AFB2023.json")
# read data2
info_data_en_AFB2023 <- fromJSON(path_data_en_AFB23)
data_en <- info_data_en_AFB2023
df <- data_en$customer_agent_discussion[!sapply(data_en$customer_agent_discussion, function(x) length(x) == 0)]
# reunion de tous les sous dataframes en un
df_total <- data.frame(key = character(), value = character(), stringsAsFactors = FALSE)
for (i in 1:length(df)){
df_total <- rbind(df_total, df[[i]])
}
# Separation des contenus en anglais de ceux en francais
file_pretrained = system.file("language_identification/lid.176.ftz", package = "fastText")
dtbl_out <- language_identification(df_total$value, file_pretrained)
indexes <- which(dtbl_out$iso_lang_1 == "fr")
df_total_fr <- df_total[indexes, ]
df_total_en <- df_total[!(seq_along(df_total$key) %in% indexes), ]
# Création d'un dataframe vide pour stocker les données nettoyées
df_total_en_clean <- data.frame(key = character(), value = character(), stringsAsFactors = FALSE)
# traitement du texte anglais
for (i in 1:nrow(df_total_en)){
# Convertir le texte en minuscules
df_total_en$value[i] <- tolower(df_total_en$value[i])
# Supprimer la ponctuation
df_total_en$value[i] <- gsub("[[:punct:]]", " ", df_total_en$value[i])
# Supprimer les chiffres
df_total_en$value[i] <- gsub("\\d+", "", df_total_en$value[i])
# Supprimer les mots vides
stopwords_list <- stopwords("en")
# Mots à ajouter à la liste des stopwords
mots_a_ajouter <- c("fichier", "jpg", "hello", "ok", "good morning","morning",
"okay", "hi", "sir", "allo", "goodmorning","good evening",
"evening","helloo")
# Concaténer les mots à ajouter avec la liste existante des stopwords
stopwords_list <- c(stopwords_list, mots_a_ajouter)
df_total_en$value[i] <- removeWords(df_total_en$value[i], stopwords_list)
#retrait des espaces en trop
df_total_en$value[i] <- gsub("\\s+"," ",df_total_en$value[i])
# Ajouter la ligne nettoyée au dataframe de données nettoyées
df_total_en_clean <- rbind(df_total_en_clean, df_total_en[i,])
}
# selection des messages en anglais ou les clients interviennent
df_total_en_clean_client <- df_total_en_clean[which(df_total_en_clean$key=="client"),]
##################################### Analuse du texte ######################
corpus <- Corpus(VectorSource(df_total_en_clean_client$value))
#retirer les espaces en trop (s'il en reste encore)
corpus <- tm_map(corpus,stripWhitespace)
#Création de la matrice documents-termes
mdt <- DocumentTermMatrix(corpus,control=list(weighting=weightBin))
m <- as.matrix(mdt)
#frequence des mots
freqMots <- colSums(m)
# conserver que les termes apparaissant plus de 2 fois dans la matrice
mClean <- m[,colSums(m) > 2]
#wordcloud des mot restant
word_freq_df <- data.frame(word = colnames(mClean), freq = colSums(mClean))
#####################################################""
route <- paste(path_data,"/Topic_modelling", sep="")
# Vérifier si le dossier existe déjà
if (!dir.exists(route)) {
# Si le dossier n'existe pas, exécuter serVis
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm(tokens(corp))
dfm = dfm_trim(dfm, min_docfreq = 5)
dtm = convert(dfm, to = "topicmodels")
set.seed(1)
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, out.dir = route, open.browser = TRUE)
} else {
# Si le dossier existe déjà, afficher un message ou effectuer une autre action
print("Le dossier Topic Modeling existe déjà, aucune action supplémentaire nécessaire.")
}
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm(tokens(corp))
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = quanteda::corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm(tokens(corp))
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm(tokens(corp))
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm( quanteda::tokens(corp))
dfm = dfm_trim(dfm, min_docfreq = 5)
dfm
dtm = convert(dfm, to = "topicmodels")
dtm
runApp()
runApp()
runApp()
lda
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "lda/index.html"
lda <- tags$iframe(src=url)
lda
lda$attribs
lda$children
lda$name
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
addResourcePath("lda", route)
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
lda$name
lda$attribs
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
output$myChart <- renderUI({
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
lda$attri})
output$myChart <- renderUI({
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
lda$attri})
lda$attri
output$myChart <- renderUI({
root <- getwd()
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
lda})
path_data <- file.path(root,"data")
route <- paste(path_data,"/Topic_modelling", sep="")
addResourcePath("lda", route)
url = "data/Topic_modelling/index.html"
lda <- tags$iframe(src=url)
lda
lda$attribs
lda$children
runApp()
write.csv(df_total_en_clean_client, file.path(path_data, "df_total_en_clean_client.csv"), row.names = FALSE)
route <- file.path(path_data, "df_total_en_clean_client.csv")
route
route <- file.path(path_data, "df_total_en_clean_client.csv")
route
#####################################################""
route <- paste(path_data,"/Topic_modelling", sep="")
route
route
route <- file.path(path_data, "df_total_en_clean_client.csv")
route
# Si le dossier existe déjà, afficher un message ou effectuer une autre action
read.csv(route)
runApp()
runApp()
route
json
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm( quanteda::tokens(corp))
dfm = dfm_trim(dfm, min_docfreq = 5)
dtm = convert(dfm, to = "topicmodels")
set.seed(1)
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
json
route <- paste(path_data,"/Topic_modelling/lda.json", sep="")
route
json <- fromJSON(route)
json
serVis(json)
serVis(json, out.dir = route, open.browser = TRUE)
serVis(json,open.browser = TRUE)
serVis(json, open.browser = TRUE)
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, open.browser = TRUE)
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, open.browser = TRUE)
json <- fromJSON(route)
serVis(json, open.browser = TRUE)
runApp()
runApp()
runApp()
runApp()
runApp()
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm( quanteda::tokens(corp))
dfm = dfm_trim(dfm, min_docfreq = 5)
dtm = convert(dfm, to = "topicmodels")
set.seed(1)
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
#####################################################""
route <- paste(path_data,"/Topic_modelling", sep="")
serVis(json, out.dir = route, open.browser = FALSE)
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, out.dir = route, open.browser = FALSE)
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, out.dir = route, open.browser = FALSE)
route
getwd()
runApp()
shiny::runApp()
shiny::runApp()
serVis(json, open.browser = TRUE)
# topic model des message des clients en anglais
sotu_corpus <- corpus(df_total_en_clean_client$value)
corp = corpus_reshape(sotu_corpus, to = "sentences")
#corp = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm( quanteda::tokens(corp))
dfm = dfm_trim(dfm, min_docfreq = 5)
dtm = convert(dfm, to = "topicmodels")
set.seed(1)
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
json = createJSON(phi = phi, theta = theta, vocab = vocab,
doc.length = doc.length, term.frequency = term.freq)
serVis(json, open.browser = TRUE)
route
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
word_freq_df
write.csv(word_freq_df, file.path(path_data, "word_freq_df.csv"), row.names = FALSE)
runApp()
##################################### Analyse du texte ######################
route <- file.path(path_data, "word_freq_df.csv")
# Vérifier si le dossier existe déjà
if (!dir.exists(route)) {
corpus <- Corpus(VectorSource(df_total_en_clean_client$value))
# print(corpus)
#retirer les espaces en trop (s'il en reste encore)
corpus <- tm_map(corpus,stripWhitespace)
#Création de la matrice documents-termes
mdt <- DocumentTermMatrix(corpus,control=list(weighting=weightBin))
m <- as.matrix(mdt)
#frequence des mots
freqMots <- colSums(m)
#wordcloud des mot restant
word_freq_df <- data.frame(word = colnames(mClean), freq = colSums(mClean))
write.csv(word_freq_df, file.path(path_data, "word_freq_df.csv"), row.names = FALSE)
} else {
# Si le dossier existe déjà, afficher un message ou effectuer une autre action
print("Le dossier Topic Modeling existe déjà, aucune action supplémentaire nécessaire.")
word_freq_df <- read.csv(route)
}
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
############## Scenario en anglais
path_data_en_AFB23 <- file.path(path_data,"en_info_data_AFB2023.json")
# read data2
info_data_en_AFB2023 <- fromJSON(path_data_en_AFB23)
data_en <- info_data_en_AFB2023
# Nettoyage des données en supprimant les valeurs manquantes
data_clean <- data_en$Scenario_Chatbot[!is.na(data_en$Scenario_Chatbot)]
# Compter les occurrences de chaque scénario
data_scenario_counts <- as.data.frame(table(unlist(data_clean)))
scenario_counts <- table(unlist(data_clean))
# Créer un dataframe à partir des résultats
scenario_df <- data.frame(Scenario = names(scenario_counts), Count = as.numeric(scenario_counts))
# Filtrer les scénarios qui commencent par un chiffre suivi de deux points
scenario_df <- scenario_df[grep("^\\d{1,}: ", scenario_df$Scenario), ]
# Réinitialiser les index du dataframe
rownames(scenario_df) <- NULL
# Retirer les chiffres, les deux points et le mot "pour" au début de chaque scénario
scenario_df$Scenario <- gsub("^(\\d{1,}: |for )", "", scenario_df$Scenario)
# Trouver les indices des scénarios contenant le terme "Tapez"
indices <- grep("Press", scenario_df$Scenario)
# Supprimer les scénarios correspondants du dataframe
scenario_df <- scenario_df[-indices, ]
# Réinitialiser les index du dataframe
rownames(scenario_df) <- NULL
# Trier les données par nombre d'occurrences
scenario_df <- scenario_df[order(scenario_df$Count, decreasing = TRUE), ]
# Créer un graphique interactif avec Plotly
plot_ly(scenario_df, x = ~Count, y = ~Scenario, type = 'bar', orientation = 'h') %>%
layout(title = "Fréquence des scénarios de chatbot",
xaxis = list(title = "Nombre d'occurrences"),
yaxis = list(title = "Scénario"))
runApp()
runApp()
runApp()
shiny::runApp()
wordcloud2(demoFreq)
wordcloud2(demoFreq)
shiny::runApp()
wordcloud2(word_freq_df)
wordcloud2(word_freq_df, size = 2)
shiny::runApp()
