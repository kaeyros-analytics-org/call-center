e_toolbox_feature() %>%
e_toolbox_feature(
feature = "magicType",
type = list("line", "bar")
) %>%
e_legend(type = "scroll",  orient = "vertical", right = "0%", top = "10%")
top_30_sorted <- top_30 %>%
arrange(desc(Count))
# Créer le graphique
top_30_sorted %>%
e_charts(Scenario) %>%
e_bar(Count) %>%
e_flip_coords() %>%
e_tooltip(trigger = "axis", axisPointer = list(type = "shadow")) %>%
e_x_axis(name = "Occurrences") %>%
e_y_axis(name = "Scénario") %>%
e_toolbox_feature() %>%
e_toolbox_feature(
feature = "magicType",
type = list("line", "bar")
) %>%
e_legend(type = "scroll", orient = "vertical", right = "0%", top = "10%")
top_30_sorted
# Créer le graphique
top_30_sorted %>%
e_charts(Scenario) %>%
e_bar(desc(Count)) %>%
e_flip_coords() %>%
e_tooltip(trigger = "axis", axisPointer = list(type = "shadow")) %>%
e_x_axis(name = "Occurrences") %>%
e_y_axis(name = "Scénario") %>%
e_toolbox_feature() %>%
e_toolbox_feature(
feature = "magicType",
type = list("line", "bar")
) %>%
e_legend(type = "scroll", orient = "vertical", right = "0%", top = "10%")
runApp()
shiny::runApp()
library(plotly)
library(wordcloud2)
########plotly####################### Analyse des scenarios ############################
############## Scenario en anglais
path_data_en_AFB23 <- file.path(path_data,"en_info_data_AFB2023.json")
# read data2
info_data_en_AFB2023 <- fromJSON(path_data_en_AFB23)
# info_data_en_AFB2023$Start_time_discusion <- ymd_hms(info_data_en_AFB2023$Start_time_discusion)
# info_data_en_AFB2023$End_time_discusion <- ymd_hms(info_data_en_AFB2023$End_time_discusion)
# info_data_en_AFB2023$Start_time_chatbot <- ymd_hms(info_data_en_AFB2023$Start_time_chatbot)
# info_data_en_AFB2023$End_time_chatbot <- ymd_hms(info_data_en_AFB2023$End_time_chatbot)
# info_data_en_AFB2023$Start_time_agent <- ymd_hms(info_data_en_AFB2023$Start_time_agent)
# info_data_en_AFB2023$End_time_agent <- ymd_hms(info_data_en_AFB2023$End_time_agent)
data_en <- info_data_en_AFB2023
# Nettoyage des données en supprimant les valeurs manquantes
data_clean <- data_en$Scenario_Chatbot[!is.na(data_en$Scenario_Chatbot)]
# Compter les occurrences de chaque scénario
data_scenario_counts <- as.data.frame(table(unlist(data_clean)))
scenario_counts <- table(unlist(data_clean))
# Créer un dataframe à partir des résultats
scenario_df <- data.frame(Scenario = names(scenario_counts), Count = as.numeric(scenario_counts))
# Réinitialiser les index du dataframe
rownames(scenario_df) <- NULL
# Retirer les chiffres, les deux points et le mot "for" au début de chaque scénario
scenario_df$Scenario <- gsub("^(\\d{1,}:)", "", scenario_df$Scenario)
# Retirer tous les "for " de la chaîne
scenario_df$Scenario <- gsub("for ", "", scenario_df$Scenario)
# Trouver les indices des scénarios contenant "Press" et "return"
indices <- grep("Press|return", scenario_df$Scenario)
# Supprimer les scénarios correspondants du dataframe
scenario_df <- scenario_df[-indices, ]
# Réinitialiser les index du dataframe
rownames(scenario_df) <- NULL
# Trier les données par nombre d'occurrences
scenario_df <- scenario_df[order(scenario_df$Count, decreasing = TRUE), ]
# Créer un graphique interactif avec Plotly
plot_ly(scenario_df, x = ~Count, y = ~Scenario, type = 'bar', orientation = 'h') %>%
layout(title = "Fréquence des scénarios de chatbot",
xaxis = list(title = "Nombre d'occurrences"),
yaxis = list(title = "Scénario"))
# Wordcloud sur les scenario
# Affichage du nuage de mots interactif avec wordcloud2
wordcloud2(scenario_df,  size = 2)
data_clean
data_clean[1:10]
# Créer une liste de paires de relations (arêtes)
edges <- list()
for (conversation in data_clean) {
previous_option <- NULL
for (option in conversation) {
if (!is.na(option)) {
if (!is.null(previous_option)) {
edges <- append(edges, list(c(previous_option, option)))
}
previous_option <- option
}
}
}
previous_option
edges
# Convertir la liste en data frame
edges_df <- do.call(rbind, edges) %>%
as.data.frame() %>%
rename(from = V1, to = V2)
edges_df
# Créer le graphe
graph <- tbl_graph(edges = edges_df, directed = TRUE)
install.packages("igraph")
install.packages("ggraph")
install.packages("tidygraph")
install.packages("tidyverse")
library(wordcloud2)
library(igraph)
library(ggraph)
library(tidygraph)
library(tidyverse)
# Créer le graphe
graph <- tbl_graph(edges = edges_df, directed = TRUE)
# Visualiser le graphe
ggraph(graph, layout = "fr") +
geom_edge_link(aes(start_cap = label_rect(node1.name), end_cap = label_rect(node2.name)), arrow = arrow(length = unit(4, 'mm')), end_shape = 'circle') +
geom_node_label(aes(label = name), size = 5, repel = TRUE, fill = 'lightblue') +
theme_void() +
ggtitle("Graphe de Connaissance des Scénarios Chatbot des Clients")
f
graph
install.packages("visNetwork")
library(visNetwork)
# Créer une liste de paires de relations (arêtes)
edges <- list()
nodes <- unique(unlist(data_clean)) %>% na.omit()
nodes_df <- data.frame(id = nodes, label = nodes)
for (conversation in data_clean) {
previous_option <- NULL
for (option in conversation) {
if (!is.na(option)) {
if (!is.null(previous_option)) {
edges <- append(edges, list(c(previous_option, option)))
}
previous_option <- option
}
}
}
# Convertir la liste en data frame
edges_df <- do.call(rbind, edges) %>%
as.data.frame() %>%
rename(from = V1, to = V2)
edges_df
# Création et visualisation du graphe interactif
visNetwork(nodes_df, edges_df) %>%
visEdges(arrows = 'to') %>%
visLayout(randomSeed = 123) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(navigationButtons = TRUE) %>%
visPhysics(stabilization = TRUE)
# Création et visualisation du graphe interactif
visNetwork(nodes_df, edges_df) %>%
visEdges(arrows = 'to') %>%
visLayout(randomSeed = 123)
# Création et visualisation du graphe interactif
visNetwork(nodes_df, edges_df)
nodes_df
edges_df
# Création et visualisation du graphe interactif
visNetwork(nodes_df, edges_df) %>%
visEdges(arrows = 'to') %>%
visLayout(randomSeed = 123) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(navigationButtons = TRUE) %>%
visPhysics(stabilization = TRUE)
# Créer une liste de paires de relations (arêtes)
edges <- list()
nodes <- unique(unlist(data_clean)) %>% na.omit()
nodes_df <- data.frame(id = nodes, name = nodes)
for (conversation in data_clean) {
previous_option <- NULL
for (option in conversation) {
if (!is.na(option)) {
if (!is.null(previous_option)) {
edges <- append(edges, list(c(previous_option, option)))
}
previous_option <- option
}
}
}
# Convertir la liste en data frame
edges_df <- do.call(rbind, edges) %>%
as.data.frame() %>%
rename(source = V1, target = V2)
# Créer les données pour les nœuds et les arêtes
nodes_list <- nodes_df %>%
mutate(category = "Option") %>%
mutate(value = 1)
nodes_list
edges_list <- edges_df %>%
mutate(value = 1)
edges_list
# Visualiser le graphe interactif avec echarts4r
nodes_list %>%
e_charts() %>%
e_graph(
nodes = nodes_list,
edges = edges_list,
layout = "force"
) %>%
e_graph_nodes(
symbol_size = 50
) %>%
e_title("Graphe de Connaissance des Scénarios Chatbot des Clients") %>%
e_tooltip() %>%
e_graph_labels(position = "inside") %>%
e_graph_edge_symbol(c("circle", "arrow")) %>%
e_graph_edge_symbol_size(c(4, 10)) %>%
e_graph_edge_label(show = TRUE, formatter = htmlwidgets::JS("function(params){return params.data.value}"))
# Visualiser le graphe interactif avec echarts4r
nodes_list %>%
e_charts() %>%
e_graph(
nodes = nodes_list,
edges = edges_list,
layout = "force"
) %>%
e_graph_nodes(
symbol_size = 50
) %>%
e_title("Graphe de Connaissance des Scénarios Chatbot des Clients") %>%
e_tooltip()
# Visualiser le graphe interactif avec echarts4r
nodes_list %>%
e_charts() %>%
e_graph(
nodes = nodes_list,
edges = edges_list,
layout = "force"
) %>%
e_graph_nodes(
) %>%
e_title("Graphe de Connaissance des Scénarios Chatbot des Clients") %>%
e_tooltip()
# Visualiser le graphe interactif avec echarts4r
chart <- nodes_list %>%
e_charts() %>%
e_graph(
nodes = nodes_list,
edges = edges_list,
layout = "force"
)
chart
e_graph_nodes(
chart
) %>%
e_title("Graphe de Connaissance des Scénarios Chatbot des Clients") %>%
e_tooltip()
# Visualiser le graphe interactif avec echarts4r
chart <- nodes_list %>%
e_charts() %>%
e_graph(
nodes = nodes_list,
edges = edges_list,
layout = "force"
)
chart
# Créer une liste de paires de relations (arêtes)
edges <- list()
nodes <- unique(unlist(data_clean)) %>% na.omit()
nodes_df <- data.frame(name = nodes, value = 1, size = 10, category = "Option", symbol = "circle")
for (conversation in data_clean) {
previous_option <- NULL
for (option in conversation) {
if (!is.na(option)) {
if (!is.null(previous_option)) {
edges <- append(edges, list(c(previous_option, option)))
}
previous_option <- option
}
}
}
# Convertir la liste en data frame
edges_df <- do.call(rbind, edges) %>%
as.data.frame() %>%
rename(source = V1, target = V2)
# Création et visualisation du graphe interactif
e_charts() %>%
e_graph(layout = "force") %>%
e_graph_nodes(nodes_df, name, value, size, category, symbol) %>%
e_graph_edges(edges_df, source, target) %>%
e_tooltip() %>%
e_title("Graphe de Connaissance des Scénarios Chatbot des Clients")
shiny::runApp()
runApp('C:/Kaeyros/chatbot_nlp/NLP_chatbot-master')
install.packages("purrrlyr")
runApp('C:/Kaeyros/chatbot_nlp/NLP_chatbot-master')
runApp('C:/Kaeyros/chatbot_nlp/NLP_chatbot-master')
runApp('C:/Kaeyros/chatbot_nlp/Turkish-Chatbot-Machine-Learning-Model-main')
install.packages("rapport")
runApp('C:/Kaeyros/chatbot_nlp/Turkish-Chatbot-Machine-Learning-Model-main')
runApp('C:/Kaeyros/chatbot_nlp/Turkish-Chatbot-Machine-Learning-Model-main')
runApp('C:/Kaeyros/chatbot_nlp/Turkish-Chatbot-Machine-Learning-Model-main')
shiny::runApp()
shiny::runApp()
df_total_en
View(df_total_en)
shiny::runApp()
combined_data
combined_data %>%
e_charts(Start_time_discusion) %>%
e_line(first_response_hours, name = "First Response Time") %>%
e_line(full_resolution_hours, name = "Full Resolution Time") %>%
e_x_axis(name = "Date") %>%
e_y_axis(name = "Time") %>%
e_tooltip(trigger = "axis") %>%
e_legend(type = "scroll",  orient = "vertical", right = "0%", top = "10%") %>%
e_toolbox_feature() %>%
e_toolbox_feature(
feature = "magicType",
type = list("line", "bar")
)
combined_data %>%
e_charts(Start_time_discusion) %>%
e_line(first_response_hours, name = "First Response Time") %>%
e_line(full_resolution_hours, name = "Full Resolution Time") %>%
e_x_axis(name = "Date") %>%
e_y_axis(name = "Time") %>%
e_legend(type = "scroll",  orient = "vertical", right = "0%", top = "10%") %>%
e_toolbox_feature() %>%
e_toolbox_feature(
feature = "magicType",
type = list("line", "bar")
)
shiny::runApp()
sentiment_scores
sentiment_data$value
sentiment_data
view
View(sentiment_data)
shiny::runApp()
install.packages("ldatuning")
runApp()
install.packages("text")
runApp()
install.packages("lsa")
runApp()
shiny::runApp()
# Charger les packages nécessaires
library(shiny)
library(quanteda)
library(topicmodels)
library(stopwords)
library(dplyr)
library(ldatuning)
library(tm)
library(text)
library(lsa)
library(dplyr)
library(stringr)
library(tidytext)
library(stopwords)
library(topicmodels)
library(jsonlite)
library(LDAvis)
library(SnowballC)
library(textstem)
library(proxy)
library(tidytext)
# Définir la liste des stopwords et mots à ajouter
stopwords_list <- stopwords::stopwords('fr', source='stopwords-iso')
mots_a_ajouter <- c("fichier", "jpg","hello", "ok", "okay", "pdf", "bonjour", "bsr","bjr", "banque", "bonsoir",
"allô", "toc", "svp", "bank", "afriland", "frs", "fr", "fcfa", "cfa", "svp", "bonsoir", "weeeehhh",
"hi", "tssssuip", "bonsoi", "aló", "héllo", "first", "afrilandfirstbank", "firstbank","okkk","xaf")
stopwords_list <- c(stopwords_list, mots_a_ajouter)
# Fonction de prétraitement pour une chaîne de caractères
preprocess_text <- function(text) {
# Convertir le texte en minuscules
text <- tolower(text)
# Remplacer la ponctuation par des espaces
text <- gsub("[[:punct:]]", " ", text)
# Retirer les chiffres
text <- gsub("\\d+", "", text)
# Tokenisation et suppression des mots vides
tokens <- unlist(strsplit(text, "\\s+"))
tokens <- tokens[!tokens %in% stopwords_list]
# Appliquer la lemmatisation (vous devez avoir une fonction de lemmatisation)
tokens <- lemmatize_words(tokens)
# Recombiner les tokens en une seule chaîne de caractères
text <- paste(tokens, collapse = " ")
return(text)
}
# Calcul de la similarité cosinus
cosine_similarity <- function(A, B) {
# Calcul des produits scalaires
AB <- A %*% t(B)
# Calcul des normes
A_norm <- sqrt(rowSums(A^2))
B_norm <- sqrt(rowSums(B^2))
# Normalisation pour obtenir la similarité cosinus
similarity <- AB / (A_norm %*% t(B_norm))
return(similarity)
}
discussion <- "mon compte sara"
discussion <- sapply(discussion , preprocess_text)
# Créer le corpus et la matrice document-fréquence (dfm)
corp_new <- corpus(discussion, docnames = paste0("new_", seq_along(discussion)))
dfm_new <- dfm(corp_new)
# Créer un objet tokens à partir du corpus
tokens_new <- tokens(corp_new)
# Créer le corpus et la matrice document-fréquence (dfm)
corp_new <- corpus(discussion, docnames = paste0("new_", seq_along(discussion)))
dfm_new <- dfm(corp_new)
# Créer un objet tokens à partir du corpus
tokens_new <- tokens(corp_new)
# Créer un objet tokens à partir du corpus
tokens_new <- corp_new |>
tokens()
packageVersion("quanteda")
# Créer le corpus et la matrice document-fréquence (dfm)
corp_new <- tm::corpus(discussion, docnames = paste0("new_", seq_along(discussion)))
data(crude, package = "tm")
crude
discussion
discussion <- texts(discussion, groups = NULL, spacer = " ")
# Créer un objet tokens à partir du corpus
tokens_new <- corpus(corp_new) |>
tokens()
# Charger le package nécessaire
library(quanteda)
# Exemple de texte de discussion
discussion <- c("This is the first document.", "This is the second document.")
# Créer un corpus avec les noms de documents
corp_new <- corpus(discussion, docnames = paste0("new_", seq_along(discussion)))
# Créer un objet tokens à partir du corpus
tokens_new <- tokens(corp_new)
# Charger les packages nécessaires
library(quanteda)
library(tm)
# Exemple de VCorpus
texts <- c("This is the first document.", "This is the second document.")
corpus_tm <- VCorpus(VectorSource(texts))
# Créer un corpus à partir du VCorpus
corp_tm <- corpus(corpus_tm)
# Créer des tokens
tokens_tm <- tokens(corp_tm)
# Charger les packages nécessaires
library(quanteda)
library(tm)
# Exemple de VCorpus
texts <- c("This is the first document.", "This is the second document.")
corpus_tm <- VCorpus(VectorSource(texts))
# Créer un corpus à partir du VCorpus
corp_tm <- corpus(corpus_tm)
# Créer une DFM
dfm_tm <- dfm(corpus_tm)
# Charger le package nécessaire
library(quanteda)
# Exemple de data.frame
df <- data.frame(
doc_id = 1:2,
text = c("This is the first document.", "This is the second document."),
stringsAsFactors = FALSE
)
# Créer un corpus à partir du data.frame
corp_new <- corpus(df, docid_field = "doc_id", text_field = "text")
# Créer des tokens
tokens_new <- tokens(corp_new)
# Créer un objet tokens à partir du corpus
tokens_new <- discussion |>
tokens()
# Créer un objet tokens à partir du corpus
tokens_new <- discussion |>
tokens() |>
dfm()
tokens_new
# Créer un objet tokens à partir du corpus
tokens_new <- discussion |>
tokens(, language = "en") |>
dfm()
library(quanteda)
# Prétraiter le texte de la discussion
print(discussion)
discussion <- "mon compte sara"
discussion <- sapply(discussion , preprocess_text)
# Créer le corpus et la matrice document-fréquence (dfm)
corp_new <- corpus(discussion, docnames = paste0("new_", seq_along(discussion)))
# Créer un objet tokens à partir du corpus
tokens_new <- discussion |>
tokens() |>
dfm()
? quanteda::tokens()
tokenizers::tokenize_words(discussion)
# Créer un objet tokens à partir du corpus
tokens_new <- tokenizers::tokenize_words(discussion) |>
tokens() |>
dfm()
# Créer un objet tokens à partir du corpus
tokens_new <- tokenizers::tokenize_words(discussion) |>
dfm()
# Créer un objet tokens à partir du corpus
tokens_new <- tokenizers::tokenize_words(discussion)
dfm(tokens_new)
dfm(tokens(tokenizers::tokenize_words(discussion)))
tokens(tokenizers::tokenize_words(discussion))
packages_info <- installed.packages()
# Extraire les répertoires d'installation des packages
library_paths <- packages_info[, "LibPath"]
library_paths
.libPaths()
shiny::runApp()
require(quanteda)
library(RcppParallel)
install.packages("RcppParallel")
require(quanteda)
runApp()
runApp()
runApp()
